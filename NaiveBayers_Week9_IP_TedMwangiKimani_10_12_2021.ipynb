{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NaiveBayers_Week9_IP_TedMwangiKimani_10/12/2021 ",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "During this weekâ€™s Independent project, you will get to test the skills that you learned this week. More specifically, you will get the test your understanding of the following learning outcomes.\n",
        "\n",
        "**Overall Learning Outcomes**\n",
        "\n",
        "I can understand and apply supervised learning algorithms such as regression, decision trees, KNN, SVM, naive Bayes, random forests to solving business problems.\n",
        "I can understand the benefits, limitations, and requirements of various supervised learning algorithms."
      ],
      "metadata": {
        "id": "g-Hi2ECahavR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimental Procedure:\n",
        "\n",
        "\n",
        "* Download the two datasets from the given links:\n",
        "\n",
        "* Randomly partition each dataset into two parts i.e 80 - 20  sets.\n",
        "\n",
        "* For dataset 2, perform classification of the testing set samples using the Naive Bayes Classifier.\n",
        "\n",
        "* Compute the accuracy (percentage of correct classification).\n",
        "\n",
        "* Report the confusion matrix of each classifier.\n",
        "\n",
        "* Repeat step 2 to step 4 twice, each time splitting the datasets differently i.e. 70-30, 60-40, then note the outcomes of your modeling.\n",
        "\n",
        "* Suggest and apply at least one of the optimization techniques that you learned earlier this week.\n",
        "\n",
        "* Provide further recommendations to improve both classifiers."
      ],
      "metadata": {
        "id": "oHFveDzjhcw4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Defining the Question\n",
        "\n",
        "* a) Specifying the Data Analytic Question\n",
        "\n",
        "\n",
        "\n",
        "* b) Defining the Metric for Success\n",
        "\n",
        "Having a model with a >70% accuracy rate\n",
        "\n",
        "* c) Understanding the context\n",
        "\n",
        "Using a naive bayers model to predict our feature variable\n",
        "\n",
        "* d) Recording the Experimental Design\n",
        "\n",
        "We will use the establish process learnt in moringa for analysis and documenting\n",
        "\n",
        "* e) Data Relevance\n",
        "\n",
        "The data is a common dataset used for learning ML techniques.Used for Classifying Email as Spam or Non-Spam"
      ],
      "metadata": {
        "id": "DjqyOKfkiHrq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lets get to it!**\n",
        "\n",
        "\n",
        "Starting with imports"
      ],
      "metadata": {
        "id": "eT3_uCurhu4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sn\n"
      ],
      "metadata": {
        "id": "aRDVmZBOhsb_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('http://bit.ly/SpamCollectionDataset', sep='\\t',  header = None, names = ['label', 'message'])\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "10cosxLChx-q",
        "outputId": "8caa79bd-c88d-4fa9-8a73-6962ee1ada48"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                            message\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have encountered this dataset before, well start with the preprocessing"
      ],
      "metadata": {
        "id": "7mEqBtMxsmCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the labels from strings to binary values for our classifier\n",
        "# \n",
        "df['label'] = df.label.map({'ham': 0, 'spam': 1})"
      ],
      "metadata": {
        "id": "WcO7lnpMsayX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting all characters in the message to lower case\n",
        "# \n",
        "df['message'] = df.message.map(lambda x: x.lower())\n",
        "\n",
        "# Removing any punctuation\n",
        "# \n",
        "df['message'] = df.message.str.replace('[^\\w\\s]', '')"
      ],
      "metadata": {
        "id": "mnocW9eis-9d"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing the messages into into single words using nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "df['message'] = df['message'].apply(nltk.word_tokenize)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHobgWN_tBHH",
        "outputId": "55b73cf7-a3d9-455e-8ac5-1efce7ef5d35"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We will perform some word stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        " \n",
        "df['message'] = df['message'].apply(lambda x: [stemmer.stem(y) for y in x])"
      ],
      "metadata": {
        "id": "Rc3-6SlutMJi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# This converts the list of words into space-separated strings\n",
        "df['message'] = df['message'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "count_vect = CountVectorizer()\n",
        "counts = count_vect.fit_transform(df['message'])"
      ],
      "metadata": {
        "id": "y5QlAymctS74"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We could leave it as the simple word-count per message, but it is better to use Term Frequency Inverse Document Frequency, more known as tf-idf\n",
        "# \n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "transformer = TfidfTransformer().fit(counts)\n",
        "\n",
        "counts = transformer.transform(counts)\n"
      ],
      "metadata": {
        "id": "-iLXbiiVtVuj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can do any visualizations on this dataset"
      ],
      "metadata": {
        "id": "eG_NnLHLtaMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "HqbssBjltl8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#Starting at 80/20\n",
        "X_train, X_test, y_train, y_test = train_test_split(counts, df['label'], test_size=0.2, random_state=69)"
      ],
      "metadata": {
        "id": "QoFBt196tXrr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "model = MultinomialNB().fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "qMSMFXVJteds"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = model.predict(X_test)\n",
        "print(np.mean(predicted == y_test))\n",
        "#very high level of accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YAyQmfrtk7-",
        "outputId": "6e3c7b50-84a2-456f-b340-07bb771498fa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9479820627802691\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attempting a different version of split\n"
      ],
      "metadata": {
        "id": "E2MNNUKUtv3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This is a 70/30 split\n",
        "X_train, X_test, y_train, y_test = train_test_split(counts, df['label'], test_size=0.3, random_state=69)\n",
        "model = MultinomialNB().fit(X_train, y_train)\n",
        "predicted = model.predict(X_test)\n",
        "print(np.mean(predicted == y_test))\n",
        "#slightly higher level of accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWh19n0Dtqlr",
        "outputId": "35794a34-479b-4280-e3cb-2af2d7cf200f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9485645933014354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This is a 60/40 split\n",
        "X_train, X_test, y_train, y_test = train_test_split(counts, df['label'], test_size=0.4, random_state=69)\n",
        "model = MultinomialNB().fit(X_train, y_train)\n",
        "predicted = model.predict(X_test)\n",
        "print(np.mean(predicted == y_test))\n",
        "#slightly lower level of accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTXkMWWduDq1",
        "outputId": "d75f0bd3-10c2-4963-b0fa-c6715d3e0f34"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9407806191117093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimization of the model"
      ],
      "metadata": {
        "id": "36j6gv55uQtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We will attempt a smoothing technique \n",
        "X_train, X_test, y_train, y_test = train_test_split(counts, df['label'], test_size=0.2, random_state=69)\n",
        "model2 = MultinomialNB(alpha=3).fit(X_train, y_train)\n",
        "predicted = model2.predict(X_test)\n",
        "print(np.mean(predicted == y_test))\n",
        "\n",
        "#This brought down the accuracy "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuuQnxRCuMli",
        "outputId": "2ec5392a-e3aa-4cbc-daeb-d7803d2a5c2f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.895067264573991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(counts, df['label'], test_size=0.2, random_state=69)\n",
        "model2 = MultinomialNB(alpha=0).fit(X_train, y_train)\n",
        "predicted = model2.predict(X_test)\n",
        "print(np.mean(predicted == y_test))\n",
        "\n",
        "#No smoothing brings back an accuracy of 98%!!!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYH-diVsx2oc",
        "outputId": "661b9cab-4b31-4195-f475-cef56e53587e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9641255605381166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py:557: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
            "  % _ALPHA_MIN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(counts, df['label'], test_size=0.2, random_state=69)\n",
        "model2 = MultinomialNB(alpha=0,fit_prior= False).fit(X_train, y_train)\n",
        "predicted = model2.predict(X_test)\n",
        "print(np.mean(predicted == y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RMgTLoiyBXR",
        "outputId": "29a0b51f-4bbe-49fd-f753-3072ca9192c1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9641255605381166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py:557: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
            "  % _ALPHA_MIN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The model is performing extremely well at a 96% accuracy which is acceptable for now**"
      ],
      "metadata": {
        "id": "Je7Bvvz4EvZv"
      }
    }
  ]
}